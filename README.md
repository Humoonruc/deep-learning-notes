

[TOC]





# 神经网络与深度学习



## 感知机

- 感知机模拟神经元，是由一个或多个参数输入（树突）产生输出（轴突）的结构。
- 感知机将输入的权重和偏置（常数项）设定为参数。
- 使用感知机可以表示与门和或门等逻辑电路。
- 异或门无法通过单层感知机来表示。使用2层感知机可以表示异或门。
- 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。加深层级一般可以提升模型的性能。
- 多层感知机（在理论上）可以表示计算机。
- 多个神经元联合工作时，就构成了神经网络。



## 激活函数 activation function

感知机的输出是一个阶跃函数。激活函数用平滑的 sigmoid 函数或 ReLU 函数对阶跃函数进行改造，然后发给下一个神经元。

为了发挥叠加层的优势，激活函数不能用线性函数。

sigmoid 函数：
$$
h(x)=\frac{1}{1+e^{-x}}
$$

ReLU（Rectified Linear Unit）函数：
$$
h(x) = max(0, x)
$$



## 模型推断：前向传播 forward propagation

### 回归问题

对于回归问题，最后一层输出层的激活函数用恒等函数。

### 分类问题

对于分类问题，最后一层输出层的激活函数 softmax 函数。要分多少类，输出层就设定多少个神经元。

人们日常思考问题，习惯定性分类，而非定量解析，所以要寻找尽可能贴合、而又易于处理的分段函数形式

激活函数将输入信号的总和转换为近似的二值输出（它的形状类似阶跃分段函数，但却是可导的）



## 训练模型：后向传播 post propagation

确定最优参数的过程。

### 损失函数 loss function

loss function: 从预测模型的==参数==到模型的衡量标准==统计量==（比如，均方误差）的映射，表示模型性能的恶劣程度。

损失函数一般为多元函数。如图，为损失函数的可视化。

![1238724-20180809214058976-398502983](http://humoon-image-hosting-service.oss-cn-beijing.aliyuncs.com/img/typora/JavaScript/1238724-20180809214058976-398502983.jpg)

### 损失函数最小化：梯度下降算法

寻找最佳参数束（使损失函数最小），不一定要用数学思维硬算解析解，也可以使用==计算思维==：从初始参数束开始，多次迭代，每次求出损失函数的梯度，然后顺着梯度的方向小幅移动参数束。

这个过程称为“训练”或“学习”。

#### 批量梯度下降（Batch Gradient Descent, BGD）

模型的衡量标准统计量，使用全部样本数据得出。迭代结果必然向全局最优点收敛。

- 优点
  - （1）对所有样本进行计算，利用矩阵进行操作，实现了并行。
  - （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

- 缺点
  - （1）当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

BGD 的迭代收敛曲线示意图可以表示如下：

![img](http://humoon-image-hosting-service.oss-cn-beijing.aliyuncs.com/img/typora/JavaScript/1238724-20180810105804742-740866130.jpg)

#### 随机梯度下降（Stochastic Gradient Descent, SGD）

用全部样本计算衡量标准统计量，计算量太大。可行的计算方法常常只能是：==每次迭代选取样本中的一个数据计算衡量标准统计量==。则向全局最优点收敛的过程将是一个随机震荡的轨迹。

- 优点
  - 运算速度快

- 缺点
  - （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  - （2）可能会收敛到局部最优。
  - （3）不易于并行实现。

SGD 的迭代收敛曲线示意图可以表示如下：

![img](http://humoon-image-hosting-service.oss-cn-beijing.aliyuncs.com/img/typora/JavaScript/1238724-20180810113116872-1679884285.jpg)

#### 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）

MBGD 是对 BGD 和 SGD 的一个折中办法。其思想是：每次迭代使用包含部分数据的小样本（称为mini-batch，小批量）计算统计量。来对参数进行更新。

可以实现运算的并行化。

#### 学习率

为了保证收敛，每次迭代移动的幅度是梯度乘以学习率 $\alpha$，这个值可取0.03, 0.1等。

这个参数是人工设定的超参数，往往要根据训练的结果进行调整。

### 误差反向传播算法

计算梯度需要求偏导数。

如果参数数量比较大，求偏导的全局计算是非常复杂的。

可以用复合函数的链式求导法则，只要保存下每一层的值，就可以代入链式法则算出某一点的导数。

误差反向传播算法使梯度下降变得可操作，从理论成为实践。





## 卷积神经网络

